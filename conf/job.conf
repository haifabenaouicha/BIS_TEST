app {
  environment = "local"
  input_base_path = "data/raw"
  output_base_path = "data/output"
  invalid_base_path = "data/errors"
  sources = {
    customers_raw = {
      input_path = ${app.input_base_path}"/customers.csv"
      format = "csv"
      options = {
        header = "true"
        delimiter = ","
      }
      primary_key = "customer_id"
      invalid_path = ${app.invalid_base_path}"/customers"
    }

    products_raw = {
      input_path = ${app.input_base_path}"/products.csv"
      format = "csv"
      options = {
        header = "true"
        delimiter = ","
      }
      primary_key = "product_id"
      invalid_path = ${app.invalid_base_path}"/products"
    }

    orders_raw = {
      input_path = ${app.input_base_path}"/orders/*.csv"
      format = "csv"
      options = {
        header = "true"
        delimiter = ","
      }
      primary_key = "product_id"
      invalid_path = ${app.invalid_base_path}"/orders"
    }
    customers = {
      output_path = ${app.output_base_path}"/customers"
      format = "delta"
    }

    products = {
      output_path = ${app.output_base_path}"/products"
      format = "delta"
    }

    top_ten_countries_for_customers = {
      output_path = ${app.output_base_path}"/top_ten_countries_for_customers"
      format = "delta"
    }
    top_three_products_max_unit_proce_drop = {
      output_path = ${app.output_base_path}"/top_three_products_max_unit_proce_drop"
      format = "delta"
    }

    revenue_per_country = {
      output_path = ${app.output_base_path}"/revenue_per_country"
      format = "delta"
    }

  }
  spark = {
    app_name = "BISApp"
    master = "local[*]"
    options = {
      # General options
      "spark.sql.debug.maxToStringFields" = 1000
      "spark.sql.adaptive.enabled" = true
      "spark.sql.adaptive.coalescePartitions.enabled" = true
      "spark.sql.adaptive.skewJoin.enabled" = true
      "spark.sql.autoBroadcastJoinThreshold" = 104857600  # 100MB
      "spark.sql.broadcastTimeout" = 120
      "spark.sql.shuffle.partitions" = 200
      "spark.default.parallelism" = 200
      "spark.sql.inMemoryColumnarStorage.compressed" = true
      "spark.sql.inMemoryColumnarStorage.batchSize" = 10000
      spark.eventLog.enabled=true
    }
  }
}